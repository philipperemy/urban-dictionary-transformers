# https://datamuni.com/@shivanandroy/transformers-generating-arxiv-ds-title-from-abstracts

import json
import logging
import sys

import pandas as pd
from simpletransformers.config.model_args import Seq2SeqArgs
from simpletransformers.seq2seq import Seq2SeqModel

logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

DATA_FILE = 'words.json'


def get_metadata():
    with open(DATA_FILE, 'r') as f:
        for line in f:
            yield line


debug = len(sys.argv) > 1
words = []
definitions = []
metadata = get_metadata()
count = 0
for i, d in enumerate(metadata):
    if debug and i > 10_000:
        break
    d_dict = json.loads(d)
    definition = d_dict['definition']
    if d_dict['thumbs_up'] > d_dict['thumbs_down'] and d_dict['thumbs_up'] > 3:
        definitions.append(definition)
        words.append(d_dict['word'])
print(count)

data = pd.DataFrame({
    'word': words,
    'definition': definitions,
})
print(data.head())

data = data[['word', 'definition']]
data.columns = ['target_text', 'input_text']

data['prefix'] = 'summarize'

# splitting the data into training and test dataset
eval_df = data.sample(frac=0.05, random_state=101)
train_df = data.drop(eval_df.index)

print(train_df.shape, eval_df.shape)

model_args = Seq2SeqArgs()
if debug:
    model_args.overwrite_output_dir = True
    model_args.num_train_epochs = 1
else:
    model_args.num_train_epochs = 1  # put back 10.
model_args.evaluate_generated_text = True
model_args.use_multiprocessing = False
model_args.use_multiprocessing_for_evaluation = False
model_args.thread_count = 1
model_args.evaluate_during_training = False
model_args.evaluate_during_training_verbose = False
model_args.evaluate_during_training_steps = int(1e9)  # we don't want.
model_args.early_stopping = False  # we don't want.
model_args.save_steps = 50_000
model_args.train_batch_size = 16

# Initialize model
model = Seq2SeqModel(
    encoder_decoder_type='bart',
    encoder_decoder_name='facebook/bart-base',
    args=model_args,
    use_cuda=True,
)

model.train_model(train_df, eval_data=eval_df)

# # Evaluate the model
results = model.eval_model(eval_df)

random_num = 350
actual_title = eval_df.iloc[random_num]['target_text']
actual_abstract = [eval_df.iloc[random_num]['input_text']]
predicted_title = model.predict(actual_abstract)

print(f'Actual Word: {actual_title}')
print(f'Predicted Word: {predicted_title}')
print(f'Actual Definition: {actual_abstract}')

# Use the model for prediction
print(
    model.predict(
        [
            "Tyson is a Cyclops, a son of Poseidon, and Percy Jacksonâ€™s half brother. He is the current general of the Cyclopes army."
        ]
    )
)

# model_args = T5Args()
# model_args.max_seq_length = 96
# model_args.train_batch_size = 20
# model_args.eval_batch_size = 20
# model_args.num_train_epochs = 1
# model_args.evaluate_during_training = True
# model_args.evaluate_during_training_steps = 30000
# model_args.use_multiprocessing = False
# model_args.fp16 = False
# model_args.save_steps = -1
# model_args.save_eval_checkpoints = False
# model_args.no_cache = True
# model_args.reprocess_input_data = True
# model_args.overwrite_output_dir = True
# model_args.preprocess_inputs = False
# model_args.num_return_sequences = 1
# model_args.use_multiprocessing_for_evaluation = True
# model_args.process_count = 2

# Create T5 Model
# model = T5Model('mt5', 't5-small', args=model_args)

# Train T5 Model on new task
# model.train_model(train_df, eval_data=eval_df)
#
# # Evaluate T5 Model on new task
# results = model.eval_model(eval_df.head(10))
#
# print(results)
#
# random_num = 350
# actual_title = eval_df.iloc[random_num]['target_text']
# actual_abstract = ['summarize: ' + eval_df.iloc[random_num]['input_text']]
# predicted_title = model.predict(actual_abstract)
#
# print(f'Actual Title: {actual_title}')
# print(f'Predicted Title: {predicted_title}')
# print(f'Actual Abstract: {actual_abstract}')
